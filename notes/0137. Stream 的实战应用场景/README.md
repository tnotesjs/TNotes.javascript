# [0137. Stream çš„å®æˆ˜åº”ç”¨åœºæ™¯](https://github.com/tnotesjs/TNotes.javascript/tree/main/notes/0137.%20Stream%20%E7%9A%84%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF)

<!-- region:toc -->

- [1. ğŸ¯ æœ¬èŠ‚å†…å®¹](#1--æœ¬èŠ‚å†…å®¹)
- [2. ğŸ«§ è¯„ä»·](#2--è¯„ä»·)
- [3. ğŸ¤” å¦‚ä½•ä½¿ç”¨æµå®ç°å¤§æ–‡ä»¶çš„åˆ†ç‰‡ä¸Šä¼  ï¼Ÿ](#3--å¦‚ä½•ä½¿ç”¨æµå®ç°å¤§æ–‡ä»¶çš„åˆ†ç‰‡ä¸Šä¼ -)
  - [3.1. åŸºæœ¬åˆ†ç‰‡ä¸Šä¼ ](#31-åŸºæœ¬åˆ†ç‰‡ä¸Šä¼ )
  - [3.2. ä½¿ç”¨æµå¼ä¸Šä¼ ](#32-ä½¿ç”¨æµå¼ä¸Šä¼ )
  - [3.3. å¸¦é‡è¯•çš„åˆ†ç‰‡ä¸Šä¼ ](#33-å¸¦é‡è¯•çš„åˆ†ç‰‡ä¸Šä¼ )
  - [3.4. å¹¶å‘åˆ†ç‰‡ä¸Šä¼ ](#34-å¹¶å‘åˆ†ç‰‡ä¸Šä¼ )
  - [3.5. ç§’ä¼ å’Œæ–­ç‚¹ç»­ä¼ ](#35-ç§’ä¼ å’Œæ–­ç‚¹ç»­ä¼ )
- [4. ğŸ¤” å¦‚ä½•å®ç°æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸‹è½½ ï¼Ÿ](#4--å¦‚ä½•å®ç°æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸‹è½½-)
  - [4.1. åŸºæœ¬æ–­ç‚¹ç»­ä¼ ](#41-åŸºæœ¬æ–­ç‚¹ç»­ä¼ )
  - [4.2. å¸¦é‡è¯•çš„æ–­ç‚¹ç»­ä¼ ](#42-å¸¦é‡è¯•çš„æ–­ç‚¹ç»­ä¼ )
  - [4.3. æµå¼å†™å…¥ä¸‹è½½](#43-æµå¼å†™å…¥ä¸‹è½½)
- [5. ğŸ¤” è§†é¢‘æµæ’­æ”¾å¦‚ä½•åˆ©ç”¨æµçš„èƒŒå‹æœºåˆ¶ ï¼Ÿ](#5--è§†é¢‘æµæ’­æ”¾å¦‚ä½•åˆ©ç”¨æµçš„èƒŒå‹æœºåˆ¶-)
  - [5.1. åŸºæœ¬è§†é¢‘æµç¼“å†²](#51-åŸºæœ¬è§†é¢‘æµç¼“å†²)
  - [5.2. è‡ªé€‚åº”ç¼“å†²ç­–ç•¥](#52-è‡ªé€‚åº”ç¼“å†²ç­–ç•¥)
  - [5.3. æ’­æ”¾è¿›åº¦ä¸ç¼“å†²åŒæ­¥](#53-æ’­æ”¾è¿›åº¦ä¸ç¼“å†²åŒæ­¥)
- [6. ğŸ¤” å¦‚ä½•æµå¼è§£æå¤§å‹ CSV æ–‡ä»¶ ï¼Ÿ](#6--å¦‚ä½•æµå¼è§£æå¤§å‹-csv-æ–‡ä»¶-)
  - [6.1. åŸºæœ¬ CSV æµå¼è§£æ](#61-åŸºæœ¬-csv-æµå¼è§£æ)
  - [6.2. å¸¦éªŒè¯çš„ CSV è§£æ](#62-å¸¦éªŒè¯çš„-csv-è§£æ)
  - [6.3. CSV è½¬æ¢å’Œèšåˆ](#63-csv-è½¬æ¢å’Œèšåˆ)
- [7. ï¿½ demos](#7--demos)
  - [7.1. demos/1: å¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ ](#71-demos1-å¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ )
  - [7.2. demos/2: æ–­ç‚¹ç»­ä¼ ä¸‹è½½](#72-demos2-æ–­ç‚¹ç»­ä¼ ä¸‹è½½)
  - [7.3. demos/3: è§†é¢‘æµèƒŒå‹æ§åˆ¶](#73-demos3-è§†é¢‘æµèƒŒå‹æ§åˆ¶)
  - [7.4. demos/4: CSV æµå¼è§£æ](#74-demos4-csv-æµå¼è§£æ)
- [8. ğŸ”— å¼•ç”¨](#8--å¼•ç”¨)

<!-- endregion:toc -->

## 1. ğŸ¯ æœ¬èŠ‚å†…å®¹

- å¤§æ–‡ä»¶ä¸Šä¼ ä¸åˆ†ç‰‡å¤„ç†
- å¤§æ–‡ä»¶ä¸‹è½½ä¸æ–­ç‚¹ç»­ä¼ 
- è§†é¢‘æµæ’­æ”¾ä¸ç¼“å†²æ§åˆ¶
- CSV æ–‡ä»¶çš„æµå¼è§£æ
- WebSocket æ•°æ®æµå¤„ç†
- æ—¥å¿—æµçš„å®æ—¶å¤„ç†

## 2. ğŸ«§ è¯„ä»·

æµå¼å¤„ç†åœ¨å®æˆ˜ä¸­è§£å†³äº†å¤§æ•°æ®é‡åœºæ™¯çš„å†…å­˜å’Œæ€§èƒ½é—®é¢˜ã€‚å¤§æ–‡ä»¶ä¸Šä¼ é€šè¿‡åˆ†ç‰‡å’Œ ReadableStream å®ç°ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨å†…å®¹ã€‚æ–­ç‚¹ç»­ä¼ åˆ©ç”¨ Range è¯·æ±‚å¤´é…åˆæµå¼ä¸‹è½½ã€‚è§†é¢‘æµåˆ©ç”¨èƒŒå‹æœºåˆ¶å¹³è¡¡ç¼“å†²åŒºï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºã€‚CSV æµå¼è§£æé€šè¿‡é€è¡Œå¤„ç†æ”¯æŒ GB çº§æ–‡ä»¶ã€‚æŒæ¡è¿™äº›å®æˆ˜åœºæ™¯èƒ½æ˜¾è‘—æå‡åº”ç”¨çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

## 3. ğŸ¤” å¦‚ä½•ä½¿ç”¨æµå®ç°å¤§æ–‡ä»¶çš„åˆ†ç‰‡ä¸Šä¼  ï¼Ÿ

å°†æ–‡ä»¶åˆ‡åˆ†æˆå¤šä¸ªåˆ†ç‰‡ï¼Œä½¿ç”¨ ReadableStream é€ç‰‡è¯»å–å¹¶ä¸Šä¼ ï¼Œé…åˆè¿›åº¦å›è°ƒå’Œé”™è¯¯é‡è¯•æœºåˆ¶ã€‚

### 3.1. åŸºæœ¬åˆ†ç‰‡ä¸Šä¼ 

```js
async function uploadFileInChunks(file, url, chunkSize = 1024 * 1024) {
  const totalChunks = Math.ceil(file.size / chunkSize)

  for (let chunkIndex = 0; chunkIndex < totalChunks; chunkIndex++) {
    const start = chunkIndex * chunkSize
    const end = Math.min(start + chunkSize, file.size)
    const chunk = file.slice(start, end)

    const formData = new FormData()
    formData.append('file', chunk)
    formData.append('chunkIndex', chunkIndex)
    formData.append('totalChunks', totalChunks)
    formData.append('fileName', file.name)

    const response = await fetch(url, {
      method: 'POST',
      body: formData,
    })

    if (!response.ok) {
      throw new Error(`åˆ†ç‰‡ ${chunkIndex} ä¸Šä¼ å¤±è´¥`)
    }

    console.log(
      `å·²ä¸Šä¼ : ${(((chunkIndex + 1) / totalChunks) * 100).toFixed(1)}%`
    )
  }

  // é€šçŸ¥æœåŠ¡å™¨åˆå¹¶
  await fetch(`${url}/merge`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ fileName: file.name, totalChunks }),
  })
}
```

### 3.2. ä½¿ç”¨æµå¼ä¸Šä¼ 

```js
function createFileUploadStream(file, onProgress) {
  const chunkSize = 64 * 1024
  let offset = 0

  return new ReadableStream({
    async pull(controller) {
      if (offset >= file.size) {
        controller.close()
        return
      }

      const chunk = file.slice(offset, offset + chunkSize)
      const arrayBuffer = await chunk.arrayBuffer()

      controller.enqueue(new Uint8Array(arrayBuffer))
      offset += chunkSize

      if (onProgress) {
        onProgress({
          loaded: offset,
          total: file.size,
          progress: (offset / file.size) * 100,
        })
      }
    },
  })
}

async function uploadWithStream(file, url) {
  const stream = createFileUploadStream(file, (progress) => {
    console.log(`ä¸Šä¼ è¿›åº¦: ${progress.progress.toFixed(1)}%`)
  })

  await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/octet-stream',
      'X-File-Name': file.name,
      'X-File-Size': file.size,
    },
    body: stream,
    duplex: 'half',
  })
}
```

### 3.3. å¸¦é‡è¯•çš„åˆ†ç‰‡ä¸Šä¼ 

```js
class ChunkedUploader {
  constructor(file, url, options = {}) {
    this.file = file
    this.url = url
    this.chunkSize = options.chunkSize || 1024 * 1024
    this.maxRetries = options.maxRetries || 3
    this.onProgress = options.onProgress
  }

  async uploadChunk(chunk, index, totalChunks) {
    let retries = 0

    while (retries < this.maxRetries) {
      try {
        const formData = new FormData()
        formData.append('file', chunk)
        formData.append('chunkIndex', index)
        formData.append('totalChunks', totalChunks)
        formData.append('fileName', this.file.name)

        const response = await fetch(this.url, {
          method: 'POST',
          body: formData,
        })

        if (!response.ok) {
          throw new Error(`HTTP ${response.status}`)
        }

        return await response.json()
      } catch (error) {
        retries++
        console.warn(
          `åˆ†ç‰‡ ${index} ä¸Šä¼ å¤±è´¥ï¼ˆå°è¯• ${retries}/${this.maxRetries}ï¼‰`
        )

        if (retries >= this.maxRetries) {
          throw new Error(`åˆ†ç‰‡ ${index} ä¸Šä¼ å¤±è´¥: ${error.message}`)
        }

        await new Promise((resolve) => setTimeout(resolve, 1000 * retries))
      }
    }
  }

  async upload() {
    const totalChunks = Math.ceil(this.file.size / this.chunkSize)

    for (let i = 0; i < totalChunks; i++) {
      const start = i * this.chunkSize
      const end = Math.min(start + this.chunkSize, this.file.size)
      const chunk = this.file.slice(start, end)

      await this.uploadChunk(chunk, i, totalChunks)

      if (this.onProgress) {
        this.onProgress({
          chunkIndex: i,
          totalChunks,
          progress: ((i + 1) / totalChunks) * 100,
        })
      }
    }

    // åˆå¹¶åˆ†ç‰‡
    await fetch(`${this.url}/merge`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        fileName: this.file.name,
        totalChunks,
        fileSize: this.file.size,
      }),
    })
  }
}

// ä½¿ç”¨
const uploader = new ChunkedUploader(file, '/api/upload', {
  chunkSize: 2 * 1024 * 1024,
  maxRetries: 3,
  onProgress: (progress) => {
    updateProgressBar(progress.progress)
  },
})

await uploader.upload()
```

### 3.4. å¹¶å‘åˆ†ç‰‡ä¸Šä¼ 

```js
async function parallelChunkUpload(file, url, concurrency = 3) {
  const chunkSize = 1024 * 1024
  const totalChunks = Math.ceil(file.size / chunkSize)
  const uploadedChunks = new Set()

  async function uploadChunk(index) {
    const start = index * chunkSize
    const end = Math.min(start + chunkSize, file.size)
    const chunk = file.slice(start, end)

    const formData = new FormData()
    formData.append('file', chunk)
    formData.append('chunkIndex', index)

    await fetch(url, { method: 'POST', body: formData })
    uploadedChunks.add(index)

    console.log(`å®Œæˆ: ${uploadedChunks.size}/${totalChunks}`)
  }

  // åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
  const tasks = []
  for (let i = 0; i < totalChunks; i++) {
    tasks.push(uploadChunk(i))

    // æ§åˆ¶å¹¶å‘æ•°
    if (tasks.length >= concurrency) {
      await Promise.race(tasks.map((t) => t.catch(() => {})))
      tasks.splice(
        tasks.findIndex((t) => uploadedChunks.has(tasks.indexOf(t))),
        1
      )
    }
  }

  await Promise.all(tasks)
}
```

### 3.5. ç§’ä¼ å’Œæ–­ç‚¹ç»­ä¼ 

```js
async function smartUpload(file, url) {
  // è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
  const hash = await calculateFileHash(file)

  // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆç§’ä¼ ï¼‰
  const checkResponse = await fetch(`${url}/check`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ hash, fileName: file.name }),
  })

  const { exists, uploadedChunks } = await checkResponse.json()

  if (exists) {
    console.log('æ–‡ä»¶å·²å­˜åœ¨ï¼Œç§’ä¼ å®Œæˆ')
    return
  }

  // æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡
  const chunkSize = 1024 * 1024
  const totalChunks = Math.ceil(file.size / chunkSize)

  for (let i = 0; i < totalChunks; i++) {
    if (uploadedChunks && uploadedChunks.includes(i)) {
      console.log(`è·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡ ${i}`)
      continue
    }

    const start = i * chunkSize
    const end = Math.min(start + chunkSize, file.size)
    const chunk = file.slice(start, end)

    await uploadChunk(chunk, i, hash)
  }

  // åˆå¹¶
  await fetch(`${url}/merge`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ hash, fileName: file.name, totalChunks }),
  })
}

async function calculateFileHash(file) {
  const chunkSize = 2 * 1024 * 1024
  const chunks = Math.ceil(file.size / chunkSize)
  const spark = new SparkMD5.ArrayBuffer()

  for (let i = 0; i < chunks; i++) {
    const chunk = file.slice(i * chunkSize, (i + 1) * chunkSize)
    const arrayBuffer = await chunk.arrayBuffer()
    spark.append(arrayBuffer)
  }

  return spark.end()
}
```

åˆ†ç‰‡ä¸Šä¼ é€‚åˆå¤§æ–‡ä»¶ï¼Œç»“åˆé‡è¯•ã€å¹¶å‘ã€ç§’ä¼ èƒ½æå‡å¯é æ€§å’Œæ•ˆç‡ã€‚

## 4. ğŸ¤” å¦‚ä½•å®ç°æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸‹è½½ ï¼Ÿ

ä½¿ç”¨ HTTP Range è¯·æ±‚å¤´è·å–éƒ¨åˆ†å†…å®¹ï¼Œè®°å½•å·²ä¸‹è½½çš„å­—èŠ‚æ•°ï¼Œä¸­æ–­åä»æ–­ç‚¹å¤„ç»§ç»­ã€‚

### 4.1. åŸºæœ¬æ–­ç‚¹ç»­ä¼ 

```js
async function resumableDownload(url, fileName) {
  const storage = localStorage
  const key = `download_${fileName}`

  // è¯»å–å·²ä¸‹è½½çš„å­—èŠ‚æ•°
  let downloadedBytes = parseInt(storage.getItem(key) || '0', 10)

  // è·å–æ–‡ä»¶æ€»å¤§å°
  const headResponse = await fetch(url, { method: 'HEAD' })
  const totalSize = parseInt(headResponse.headers.get('Content-Length'), 10)

  if (downloadedBytes >= totalSize) {
    console.log('æ–‡ä»¶å·²ä¸‹è½½å®Œæˆ')
    return
  }

  console.log(`ä» ${downloadedBytes} å­—èŠ‚å¤„ç»§ç»­ä¸‹è½½`)

  // ä½¿ç”¨ Range è¯·æ±‚
  const response = await fetch(url, {
    headers: {
      Range: `bytes=${downloadedBytes}-`,
    },
  })

  const reader = response.body.getReader()
  const chunks = []

  try {
    while (true) {
      const { done, value } = await reader.read()

      if (done) {
        console.log('ä¸‹è½½å®Œæˆ')
        storage.removeItem(key)
        break
      }

      chunks.push(value)
      downloadedBytes += value.length

      // ä¿å­˜è¿›åº¦
      storage.setItem(key, downloadedBytes.toString())

      console.log(`è¿›åº¦: ${((downloadedBytes / totalSize) * 100).toFixed(1)}%`)
    }

    // åˆå¹¶æ•°æ®
    const blob = new Blob(chunks)
    const link = document.createElement('a')
    link.href = URL.createObjectURL(blob)
    link.download = fileName
    link.click()
  } catch (error) {
    console.error('ä¸‹è½½ä¸­æ–­:', error)
    console.log(`å·²ä¿å­˜è¿›åº¦: ${downloadedBytes} å­—èŠ‚`)
  }
}
```

### 4.2. å¸¦é‡è¯•çš„æ–­ç‚¹ç»­ä¼ 

```js
class ResumableDownloader {
  constructor(url, fileName, options = {}) {
    this.url = url
    this.fileName = fileName
    this.chunkSize = options.chunkSize || 1024 * 1024
    this.maxRetries = options.maxRetries || 3
    this.onProgress = options.onProgress
    this.storageKey = `download_${fileName}`
  }

  async getFileSize() {
    const response = await fetch(this.url, { method: 'HEAD' })
    return parseInt(response.headers.get('Content-Length'), 10)
  }

  getDownloadedBytes() {
    return parseInt(localStorage.getItem(this.storageKey) || '0', 10)
  }

  saveProgress(bytes) {
    localStorage.setItem(this.storageKey, bytes.toString())
  }

  clearProgress() {
    localStorage.removeItem(this.storageKey)
  }

  async downloadChunk(start, end) {
    let retries = 0

    while (retries < this.maxRetries) {
      try {
        const response = await fetch(this.url, {
          headers: { Range: `bytes=${start}-${end}` },
        })

        if (!response.ok) {
          throw new Error(`HTTP ${response.status}`)
        }

        return await response.arrayBuffer()
      } catch (error) {
        retries++
        console.warn(`ä¸‹è½½å¤±è´¥ï¼ˆå°è¯• ${retries}/${this.maxRetries}ï¼‰`)

        if (retries >= this.maxRetries) {
          throw error
        }

        await new Promise((resolve) => setTimeout(resolve, 1000 * retries))
      }
    }
  }

  async download() {
    const totalSize = await this.getFileSize()
    let downloadedBytes = this.getDownloadedBytes()

    const chunks = []

    while (downloadedBytes < totalSize) {
      const end = Math.min(downloadedBytes + this.chunkSize - 1, totalSize - 1)

      const chunk = await this.downloadChunk(downloadedBytes, end)
      chunks.push(chunk)

      downloadedBytes += chunk.byteLength
      this.saveProgress(downloadedBytes)

      if (this.onProgress) {
        this.onProgress({
          loaded: downloadedBytes,
          total: totalSize,
          progress: (downloadedBytes / totalSize) * 100,
        })
      }
    }

    this.clearProgress()

    // ä¿å­˜æ–‡ä»¶
    const blob = new Blob(chunks)
    const link = document.createElement('a')
    link.href = URL.createObjectURL(blob)
    link.download = this.fileName
    link.click()
  }
}

// ä½¿ç”¨
const downloader = new ResumableDownloader(
  'https://example.com/large-file.zip',
  'file.zip',
  {
    chunkSize: 2 * 1024 * 1024,
    maxRetries: 3,
    onProgress: (progress) => {
      console.log(`ä¸‹è½½è¿›åº¦: ${progress.progress.toFixed(1)}%`)
    },
  }
)

await downloader.download()
```

### 4.3. æµå¼å†™å…¥ä¸‹è½½

```js
async function streamDownload(url, fileName) {
  const response = await fetch(url)
  const totalSize = parseInt(response.headers.get('Content-Length'), 10)

  // ä½¿ç”¨ File System Access API
  const fileHandle = await window.showSaveFilePicker({
    suggestedName: fileName,
  })

  const writable = await fileHandle.createWritable()
  let downloaded = 0

  await response.body
    .pipeThrough(
      new TransformStream({
        transform(chunk, controller) {
          downloaded += chunk.byteLength
          console.log(`è¿›åº¦: ${((downloaded / totalSize) * 100).toFixed(1)}%`)
          controller.enqueue(chunk)
        },
      })
    )
    .pipeTo(writable)

  console.log('ä¸‹è½½å®Œæˆ')
}
```

æ–­ç‚¹ç»­ä¼ çš„æ ¸å¿ƒæ˜¯ Range è¯·æ±‚å’Œè¿›åº¦ä¿å­˜ï¼Œç¡®ä¿ç½‘ç»œä¸­æ–­åèƒ½ç»§ç»­ä¸‹è½½ã€‚

## 5. ğŸ¤” è§†é¢‘æµæ’­æ”¾å¦‚ä½•åˆ©ç”¨æµçš„èƒŒå‹æœºåˆ¶ ï¼Ÿ

é€šè¿‡ç›‘æ§ç¼“å†²åŒºå¤§å°ï¼Œä½¿ç”¨èƒŒå‹ä¿¡å·æ§åˆ¶æ•°æ®æ‹‰å–é€Ÿåº¦ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºå’Œæ’­æ”¾å¡é¡¿ã€‚

### 5.1. åŸºæœ¬è§†é¢‘æµç¼“å†²

```js
class VideoStreamPlayer {
  constructor(videoElement, streamUrl) {
    this.video = videoElement
    this.streamUrl = streamUrl
    this.buffer = []
    this.bufferSize = 0
    this.maxBufferSize = 5 * 1024 * 1024 // 5MB
    this.mediaSource = new MediaSource()
    this.sourceBuffer = null
  }

  async init() {
    this.video.src = URL.createObjectURL(this.mediaSource)

    await new Promise((resolve) => {
      this.mediaSource.addEventListener('sourceopen', resolve, { once: true })
    })

    this.sourceBuffer = this.mediaSource.addSourceBuffer(
      'video/mp4; codecs="avc1.64001f"'
    )

    this.sourceBuffer.addEventListener('updateend', () => {
      this.processBuffer()
    })
  }

  async play() {
    const response = await fetch(this.streamUrl)

    await response.body
      .pipeThrough(
        new TransformStream({
          transform: (chunk, controller) => {
            this.bufferSize += chunk.byteLength

            // èƒŒå‹æ§åˆ¶
            if (this.bufferSize > this.maxBufferSize) {
              console.log('ç¼“å†²åŒºæ»¡ï¼Œæš‚åœæ‹‰å–')
              return new Promise((resolve) => {
                const checkBuffer = () => {
                  if (this.bufferSize < this.maxBufferSize / 2) {
                    console.log('ç¼“å†²åŒºå……è¶³ï¼Œç»§ç»­æ‹‰å–')
                    resolve()
                  } else {
                    setTimeout(checkBuffer, 100)
                  }
                }
                checkBuffer()
              }).then(() => controller.enqueue(chunk))
            }

            controller.enqueue(chunk)
          },
        })
      )
      .pipeTo(
        new WritableStream({
          write: (chunk) => {
            this.buffer.push(chunk)
            this.processBuffer()
          },
        })
      )
  }

  processBuffer() {
    if (this.sourceBuffer.updating || this.buffer.length === 0) {
      return
    }

    const chunk = this.buffer.shift()
    this.bufferSize -= chunk.byteLength

    this.sourceBuffer.appendBuffer(chunk)
  }
}
```

### 5.2. è‡ªé€‚åº”ç¼“å†²ç­–ç•¥

```js
class AdaptiveVideoBuffer {
  constructor() {
    this.targetBufferSize = 3 * 1024 * 1024 // 3MB ç›®æ ‡
    this.minBufferSize = 1 * 1024 * 1024 // 1MB æœ€å°
    this.maxBufferSize = 10 * 1024 * 1024 // 10MB æœ€å¤§
    this.currentBufferSize = 0
    this.networkSpeed = 0
  }

  updateNetworkSpeed(bytesReceived, timeElapsed) {
    this.networkSpeed = bytesReceived / timeElapsed
  }

  shouldPullData() {
    // æ ¹æ®å½“å‰ç¼“å†²å’Œç½‘ç»œé€Ÿåº¦å†³å®šæ˜¯å¦æ‹‰å–
    if (this.currentBufferSize < this.minBufferSize) {
      return true
    }

    if (this.currentBufferSize > this.maxBufferSize) {
      return false
    }

    // åŠ¨æ€è°ƒæ•´ç›®æ ‡ç¼“å†²å¤§å°
    const adaptiveTarget = Math.min(this.networkSpeed * 5, this.maxBufferSize)

    return this.currentBufferSize < adaptiveTarget
  }

  createStream(sourceStream) {
    let lastPullTime = Date.now()
    let pulledBytes = 0

    return sourceStream.pipeThrough(
      new TransformStream({
        transform: async (chunk, controller) => {
          this.currentBufferSize += chunk.byteLength
          pulledBytes += chunk.byteLength

          const now = Date.now()
          const elapsed = (now - lastPullTime) / 1000

          if (elapsed > 1) {
            this.updateNetworkSpeed(pulledBytes, elapsed)
            pulledBytes = 0
            lastPullTime = now
          }

          // èƒŒå‹æ§åˆ¶
          if (!this.shouldPullData()) {
            await new Promise((resolve) => {
              const check = () => {
                if (this.shouldPullData()) {
                  resolve()
                } else {
                  setTimeout(check, 100)
                }
              }
              check()
            })
          }

          controller.enqueue(chunk)
        },
      })
    )
  }
}
```

### 5.3. æ’­æ”¾è¿›åº¦ä¸ç¼“å†²åŒæ­¥

```js
class SyncedVideoPlayer {
  constructor(videoElement) {
    this.video = videoElement
    this.bufferInfo = {
      buffered: 0,
      duration: 0,
    }
  }

  updateBufferInfo() {
    if (this.video.buffered.length > 0) {
      const buffered = this.video.buffered.end(this.video.buffered.length - 1)
      const current = this.video.currentTime
      this.bufferInfo.buffered = buffered - current
      this.bufferInfo.duration = this.video.duration
    }
  }

  getBufferHealth() {
    this.updateBufferInfo()

    if (this.bufferInfo.buffered < 2) {
      return 'critical' // éœ€è¦ç´§æ€¥åŠ è½½
    } else if (this.bufferInfo.buffered < 5) {
      return 'low' // éœ€è¦åŠ è½½
    } else if (this.bufferInfo.buffered < 10) {
      return 'good' // æ­£å¸¸
    } else {
      return 'full' // æš‚åœåŠ è½½
    }
  }

  shouldLoadMore() {
    const health = this.getBufferHealth()
    return health === 'critical' || health === 'low'
  }
}
```

èƒŒå‹æœºåˆ¶ç¡®ä¿è§†é¢‘æ’­æ”¾æµç•…ï¼Œé¿å…è¿‡åº¦ç¼“å†²å¯¼è‡´å†…å­˜é—®é¢˜ã€‚

## 6. ğŸ¤” å¦‚ä½•æµå¼è§£æå¤§å‹ CSV æ–‡ä»¶ ï¼Ÿ

ä½¿ç”¨ TransformStream é€è¡Œè§£æï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜ã€‚

### 6.1. åŸºæœ¬ CSV æµå¼è§£æ

```js
function createCSVParser() {
  let buffer = ''
  let headers = null
  let lineNumber = 0

  return new TransformStream({
    transform(chunk, controller) {
      buffer += chunk

      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        lineNumber++

        if (!line.trim()) continue

        const values = line.split(',').map((v) => v.trim())

        if (!headers) {
          headers = values
          controller.enqueue({ type: 'header', headers, lineNumber })
        } else {
          const row = {}
          headers.forEach((header, index) => {
            row[header] = values[index] || ''
          })
          controller.enqueue({ type: 'row', data: row, lineNumber })
        }
      }
    },

    flush(controller) {
      if (buffer.trim()) {
        const values = buffer.split(',').map((v) => v.trim())
        if (headers) {
          const row = {}
          headers.forEach((header, index) => {
            row[header] = values[index] || ''
          })
          controller.enqueue({
            type: 'row',
            data: row,
            lineNumber: lineNumber + 1,
          })
        }
      }
    },
  })
}

// ä½¿ç”¨
async function parseCSV(file) {
  const stream = file.stream()

  await stream
    .pipeThrough(new TextDecoderStream())
    .pipeThrough(createCSVParser())
    .pipeTo(
      new WritableStream({
        write(item) {
          if (item.type === 'header') {
            console.log('åˆ—å:', item.headers)
          } else {
            console.log('è¡Œæ•°æ®:', item.data)
          }
        },
      })
    )
}
```

### 6.2. å¸¦éªŒè¯çš„ CSV è§£æ

```js
function createValidatingCSVParser(schema) {
  let headers = null

  return new TransformStream({
    transform(item, controller) {
      if (item.type === 'header') {
        headers = item.headers
        controller.enqueue(item)
        return
      }

      const errors = []

      // éªŒè¯å¿…å¡«å­—æ®µ
      schema.required?.forEach((field) => {
        if (!item.data[field] || item.data[field].trim() === '') {
          errors.push(`ç¼ºå°‘å¿…å¡«å­—æ®µ: ${field}`)
        }
      })

      // éªŒè¯æ•°æ®ç±»å‹
      Object.entries(schema.types || {}).forEach(([field, type]) => {
        const value = item.data[field]

        if (type === 'number' && isNaN(Number(value))) {
          errors.push(`${field} å¿…é¡»æ˜¯æ•°å­—`)
        } else if (type === 'email' && !/@/.test(value)) {
          errors.push(`${field} å¿…é¡»æ˜¯æœ‰æ•ˆé‚®ç®±`)
        }
      })

      if (errors.length > 0) {
        controller.enqueue({
          type: 'error',
          lineNumber: item.lineNumber,
          errors,
          data: item.data,
        })
      } else {
        controller.enqueue(item)
      }
    },
  })
}

// ä½¿ç”¨
const schema = {
  required: ['name', 'email'],
  types: {
    age: 'number',
    email: 'email',
  },
}

await stream
  .pipeThrough(new TextDecoderStream())
  .pipeThrough(createCSVParser())
  .pipeThrough(createValidatingCSVParser(schema))
  .pipeTo(
    new WritableStream({
      write(item) {
        if (item.type === 'error') {
          console.error(`è¡Œ ${item.lineNumber} é”™è¯¯:`, item.errors)
        } else if (item.type === 'row') {
          saveToDatabase(item.data)
        }
      },
    })
  )
```

### 6.3. CSV è½¬æ¢å’Œèšåˆ

```js
function createCSVTransformer(transformFn) {
  return new TransformStream({
    transform(item, controller) {
      if (item.type === 'row') {
        try {
          const transformed = transformFn(item.data)
          controller.enqueue({ ...item, data: transformed })
        } catch (error) {
          controller.enqueue({
            type: 'error',
            lineNumber: item.lineNumber,
            errors: [error.message],
            data: item.data,
          })
        }
      } else {
        controller.enqueue(item)
      }
    },
  })
}

function createCSVAggregator() {
  const stats = {
    totalRows: 0,
    validRows: 0,
    errorRows: 0,
    fieldStats: {},
  }

  return new TransformStream({
    transform(item, controller) {
      if (item.type === 'row') {
        stats.totalRows++
        stats.validRows++

        Object.entries(item.data).forEach(([key, value]) => {
          if (!stats.fieldStats[key]) {
            stats.fieldStats[key] = { count: 0, empty: 0 }
          }
          stats.fieldStats[key].count++
          if (!value || value.trim() === '') {
            stats.fieldStats[key].empty++
          }
        })
      } else if (item.type === 'error') {
        stats.totalRows++
        stats.errorRows++
      }

      controller.enqueue(item)
    },

    flush(controller) {
      controller.enqueue({ type: 'summary', stats })
    },
  })
}

// ä½¿ç”¨
await stream
  .pipeThrough(new TextDecoderStream())
  .pipeThrough(createCSVParser())
  .pipeThrough(
    createCSVTransformer((row) => ({
      ...row,
      age: parseInt(row.age, 10),
      createdAt: new Date().toISOString(),
    }))
  )
  .pipeThrough(createCSVAggregator())
  .pipeTo(
    new WritableStream({
      write(item) {
        if (item.type === 'summary') {
          console.log('ç»Ÿè®¡ä¿¡æ¯:', item.stats)
        } else if (item.type === 'row') {
          processRow(item.data)
        }
      },
    })
  )
```

æµå¼ CSV è§£æé€‚åˆå¤„ç† GB çº§æ–‡ä»¶ï¼Œå†…å­˜å ç”¨æ’å®šã€‚

## 7. ï¿½ demos

### 7.1. demos/1: å¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ 

å®Œæ•´çš„æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ ç³»ç»Ÿï¼Œæ”¯æŒï¼š

- åˆ†ç‰‡å¤§å°é…ç½® (256KB - 5MB)
- å¹¶å‘ä¸Šä¼ æ§åˆ¶ (1-10 ä¸ªå¹¶å‘)
- å¤±è´¥è‡ªåŠ¨é‡è¯•æœºåˆ¶
- ä¸Šä¼ è¿›åº¦å¯è§†åŒ–
- åˆ†ç‰‡çŠ¶æ€ç½‘æ ¼æ˜¾ç¤º
- æš‚åœ/ç»§ç»­/å–æ¶ˆåŠŸèƒ½
- æ¨¡æ‹Ÿç½‘ç»œå¤±è´¥æµ‹è¯•

æ‰“å¼€ [demos/1/index.html](demos/1/index.html) æŸ¥çœ‹æ¼”ç¤ºã€‚

### 7.2. demos/2: æ–­ç‚¹ç»­ä¼ ä¸‹è½½

æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æ–‡ä»¶ä¸‹è½½ç®¡ç†å™¨ï¼ŒåŒ…å«ï¼š

- å¤šæ–‡ä»¶åŒæ—¶ä¸‹è½½
- Range è¯·æ±‚æ–­ç‚¹ç»­ä¼ 
- è¿›åº¦ä¿å­˜åˆ° LocalStorage
- åˆ†ç‰‡å¹¶å‘ä¸‹è½½
- ä¸‹è½½é€Ÿåº¦å’Œå‰©ä½™æ—¶é—´æ˜¾ç¤º
- åˆ†ç‰‡çŠ¶æ€å¯è§†åŒ–
- æš‚åœ/ç»§ç»­/å–æ¶ˆ/æ¸…é™¤åŠŸèƒ½

æ‰“å¼€ [demos/2/index.html](demos/2/index.html) æŸ¥çœ‹æ¼”ç¤ºã€‚

### 7.3. demos/3: è§†é¢‘æµèƒŒå‹æ§åˆ¶

å®æ—¶è§†é¢‘æµæ’­æ”¾çš„èƒŒå‹æœºåˆ¶æ¼”ç¤ºï¼Œå±•ç¤ºï¼š

- è§†é¢‘å¸§æµå¼ç”Ÿæˆå’Œæ¸²æŸ“
- ç¼“å†²åŒºå¤§å°åŠ¨æ€æ§åˆ¶
- èƒŒå‹ä¿¡å·æ¿€æ´»æœºåˆ¶
- ç¼“å†²åŒºå¥åº·åº¦ç›‘æ§
- æ‹‰å–é€Ÿç‡å’Œæ¶ˆè´¹é€Ÿç‡å¯¹æ¯”
- ç¼“å†²åŒºå†å²å›¾è¡¨
- å¯è°ƒèŠ‚ FPS å’Œç¼“å†²å‚æ•°

æ‰“å¼€ [demos/3/index.html](demos/3/index.html) æŸ¥çœ‹æ¼”ç¤ºã€‚

### 7.4. demos/4: CSV æµå¼è§£æ

å¤§å‹ CSV æ–‡ä»¶çš„æµå¼è§£æå·¥å…·ï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š

- é€è¡Œæµå¼è§£æï¼Œå†…å­˜å ç”¨æ’å®š
- æ”¯æŒ 10 ä¸‡è¡Œæµ‹è¯•æ•°æ®ç”Ÿæˆ
- æ•°æ®éªŒè¯ï¼ˆåŸºç¡€/ä¸¥æ ¼æ¨¡å¼ï¼‰
- å®æ—¶æ•°æ®é¢„è§ˆ
- è§£æé€Ÿåº¦ç»Ÿè®¡
- é”™è¯¯è¡Œæ ‡è®°å’Œæ—¥å¿—
- å¯¼å‡ºä¸º JSON æ ¼å¼
- æš‚åœ/ç»§ç»­/åœæ­¢åŠŸèƒ½

æ‰“å¼€ [demos/4/index.html](demos/4/index.html) æŸ¥çœ‹æ¼”ç¤ºã€‚

## 8. ğŸ”— å¼•ç”¨

- [Using Readable Streams - MDN][1]
- [Streams API Concepts - MDN][2]
- [File API - MDN][3]
- [Fetch API - MDN][4]

[1]: https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Using_readable_streams
[2]: https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Concepts
[3]: https://developer.mozilla.org/en-US/docs/Web/API/File_API
[4]: https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API
